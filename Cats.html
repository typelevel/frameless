
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Using Cats with RDDs Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="TypedML.html" />
    
    
    <link rel="prev" href="Job.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="FeatureOverview.html">
            
                <a href="FeatureOverview.html">
            
                    
                    TypedDataset: Feature Overview
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="TypedDatasetVsSparkDataset.html">
            
                <a href="TypedDatasetVsSparkDataset.html">
            
                    
                    Comparing TypedDatasets with Spark's Datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="TypedEncoder.html">
            
                <a href="TypedEncoder.html">
            
                    
                    Typed Encoders in Frameless
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="Injection.html">
            
                <a href="Injection.html">
            
                    
                    Injection: Creating Custom Encoders
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="Job.html">
            
                <a href="Job.html">
            
                    
                    Job[A]
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.7" data-path="Cats.html">
            
                <a href="Cats.html">
            
                    
                    Using Cats with RDDs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="TypedML.html">
            
                <a href="TypedML.html">
            
                    
                    Using Spark ML with TypedDataset
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="TypedDataFrame.html">
            
                <a href="TypedDataFrame.html">
            
                    
                    Proof of Concept: TypedDataFrame
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >Using Cats with RDDs</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="using-cats-with-frameless">Using Cats with Frameless</h1>
<p>There are two main parts to the <code>cats</code> integration offered by Frameless:</p>
<ul>
<li>effect suspension in <code>TypedDataset</code> using <code>cats-effect</code> and <code>cats-mtl</code></li>
<li><code>RDD</code> enhancements using algebraic typeclasses in <code>cats-kernel</code></li>
</ul>
<p>All the examples below assume you have previously imported <code>cats.implicits</code> and <code>frameless.cats.implicits</code>.</p>
<p><em>Note that you should not import <code>frameless.syntax._</code> together with <code>frameless.cats.implicits._</code>.</em></p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> cats.implicits._
<span class="hljs-comment">// import cats.implicits._</span>

<span class="hljs-keyword">import</span> frameless.cats.implicits._
<span class="hljs-comment">// import frameless.cats.implicits._</span>
</code></pre>
<h2 id="effect-suspension-in-typed-datasets">Effect Suspension in typed datasets</h2>
<p>As noted in the section about <code>Job</code>, all operations on <code>TypedDataset</code> are lazy. The results of 
operations that would normally block on plain Spark APIs are wrapped in a type constructor <code>F[_]</code>, 
for which there exists an instance of <code>SparkDelay[F]</code>. This typeclass represents the operation of 
delaying a computation and capturing an implicit <code>SparkSession</code>. </p>
<p>In the <code>cats</code> module, we utilize the typeclasses from <code>cats-effect</code> for abstracting over these 
effect types - namely, we provide an implicit <code>SparkDelay</code> instance for all <code>F[_]</code> for which exists
an instance of <code>cats.effect.Sync[F]</code>.</p>
<p>This allows one to run operations on <code>TypedDataset</code> in an existing monad stack. For example, given
this pre-existing monad stack:</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> frameless.<span class="hljs-type">TypedDataset</span>
<span class="hljs-comment">// import frameless.TypedDataset</span>

<span class="hljs-keyword">import</span> cats.data.<span class="hljs-type">ReaderT</span>
<span class="hljs-comment">// import cats.data.ReaderT</span>

<span class="hljs-keyword">import</span> cats.effect.<span class="hljs-type">IO</span>
<span class="hljs-comment">// import cats.effect.IO</span>

<span class="hljs-keyword">import</span> cats.effect.implicits._
<span class="hljs-comment">// import cats.effect.implicits._</span>

<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">Action</span>[<span class="hljs-type">T</span>] </span>= <span class="hljs-type">ReaderT</span>[<span class="hljs-type">IO</span>, <span class="hljs-type">SparkSession</span>, <span class="hljs-type">T</span>]
<span class="hljs-comment">// defined type alias Action</span>
</code></pre>
<p>We will be able to request that values from <code>TypedDataset</code> will be suspended in this stack:</p>
<pre><code class="lang-scala"><span class="hljs-keyword">val</span> typedDs = <span class="hljs-type">TypedDataset</span>.create(<span class="hljs-type">Seq</span>((<span class="hljs-number">1</span>, <span class="hljs-string">&quot;string&quot;</span>), (<span class="hljs-number">2</span>, <span class="hljs-string">&quot;another&quot;</span>)))
<span class="hljs-comment">// typedDs: frameless.TypedDataset[(Int, String)] = [_1: int, _2: string]</span>

<span class="hljs-keyword">val</span> result: <span class="hljs-type">Action</span>[(<span class="hljs-type">Seq</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">String</span>)], <span class="hljs-type">Long</span>)] = <span class="hljs-keyword">for</span> {
  sample &lt;- typedDs.take(<span class="hljs-number">1</span>)
  count &lt;- typedDs.count()
} <span class="hljs-keyword">yield</span> (sample, count)
<span class="hljs-comment">// result: Action[(Seq[(Int, String)], Long)] = Kleisli(&lt;function1&gt;)</span>
</code></pre>
<p>As with <code>Job</code>, note that nothing has been run yet. The effect has been properly suspended. To
run our program, we must first supply the <code>SparkSession</code> to the <code>ReaderT</code> layer and then
run the <code>IO</code> effect:</p>
<pre><code class="lang-scala">result.run(spark).unsafeRunSync()
<span class="hljs-comment">// res5: (Seq[(Int, String)], Long) = (WrappedArray((1,string)),2)</span>
</code></pre>
<h3 id="convenience-methods-for-modifying-spark-thread-local-variables">Convenience methods for modifying Spark thread-local variables</h3>
<p>The <code>frameless.cats.implicits._</code> import also provides some syntax enrichments for any monad
stack that has the same capabilities as <code>Action</code> above. Namely, the ability to provide an
instance of <code>SparkSession</code> and the ability to suspend effects.</p>
<p>For these to work, we will need to import the implicit machinery from the <code>cats-mtl</code> library:</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> cats.mtl.implicits._
<span class="hljs-comment">// import cats.mtl.implicits._</span>
</code></pre>
<p>And now, we can set the description for the computation being run:</p>
<pre><code class="lang-scala"><span class="hljs-keyword">val</span> resultWithDescription: <span class="hljs-type">Action</span>[(<span class="hljs-type">Seq</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">String</span>)], <span class="hljs-type">Long</span>)] = <span class="hljs-keyword">for</span> {
  r &lt;- result.withDescription(<span class="hljs-string">&quot;fancy cats&quot;</span>)
  session &lt;- <span class="hljs-type">ReaderT</span>.ask[<span class="hljs-type">IO</span>, <span class="hljs-type">SparkSession</span>]
  _ &lt;- <span class="hljs-type">ReaderT</span>.liftF {
         <span class="hljs-type">IO</span> {
           println(<span class="hljs-string">s&quot;Description: <span class="hljs-subst">${session.sparkContext.getLocalProperty(&quot;spark.job.description&quot;)}</span>&quot;</span>)
         }
       }
} <span class="hljs-keyword">yield</span> r
<span class="hljs-comment">// resultWithDescription: Action[(Seq[(Int, String)], Long)] = Kleisli(&lt;function1&gt;)</span>

resultWithDescription.run(spark).unsafeRunSync()
<span class="hljs-comment">// Description: fancy cats</span>
<span class="hljs-comment">// res6: (Seq[(Int, String)], Long) = (WrappedArray((1,string)),2)</span>
</code></pre>
<h2 id="using-algebraic-typeclasses-from-cats-with-rdds">Using algebraic typeclasses from Cats with RDDs</h2>
<p>Data aggregation is one of the most important operations when working with Spark (and data in general).
For example, we often have to compute the <code>min</code>, <code>max</code>, <code>avg</code>, etc. from a set of columns grouped by
different predicates. This section shows how <strong>cats</strong> simplifies these tasks in Spark by
leveraging a large collection of Type Classes for ordering and aggregating data.</p>
<p>Cats offers ways to sort and aggregate tuples of arbitrary arity.</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> frameless.cats.implicits._
<span class="hljs-comment">// import frameless.cats.implicits._</span>

<span class="hljs-keyword">val</span> data: <span class="hljs-type">RDD</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">Int</span>, <span class="hljs-type">Int</span>)] = sc.makeRDD((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>) :: (<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>) :: (<span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>) :: <span class="hljs-type">Nil</span>)
<span class="hljs-comment">// data: org.apache.spark.rdd.RDD[(Int, Int, Int)] = ParallelCollectionRDD[14] at makeRDD at &lt;console&gt;:44</span>

println(data.csum)
<span class="hljs-comment">// (10,9,9)</span>

println(data.cmax)
<span class="hljs-comment">// (8,2,3)</span>

println(data.cmin)
<span class="hljs-comment">// (1,2,3)</span>
</code></pre>
<p>In case the RDD is empty, the <code>csum</code>, <code>cmax</code> and <code>cmin</code> will use the default values for the type of
elements inside the RDD. There are counterpart operations to those that have an <code>Option</code> return type
to deal with the case of an empty RDD:</p>
<pre><code class="lang-scala"><span class="hljs-keyword">val</span> data: <span class="hljs-type">RDD</span>[(<span class="hljs-type">Int</span>, <span class="hljs-type">Int</span>, <span class="hljs-type">Int</span>)] = sc.emptyRDD
<span class="hljs-comment">// data: org.apache.spark.rdd.RDD[(Int, Int, Int)] = EmptyRDD[15] at emptyRDD at &lt;console&gt;:44</span>

println(data.csum)
<span class="hljs-comment">// (0,0,0)</span>

println(data.csumOption)
<span class="hljs-comment">// None</span>

println(data.cmax)
<span class="hljs-comment">// (0,0,0)</span>

println(data.cmaxOption)
<span class="hljs-comment">// None</span>

println(data.cmin)
<span class="hljs-comment">// (0,0,0)</span>

println(data.cminOption)
<span class="hljs-comment">// None</span>
</code></pre>
<p>The following example aggregates all the elements with a common key.</p>
<pre><code class="lang-scala"><span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">User</span> </span>= <span class="hljs-type">String</span>
<span class="hljs-comment">// defined type alias User</span>

<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">TransactionCount</span> </span>= <span class="hljs-type">Int</span>
<span class="hljs-comment">// defined type alias TransactionCount</span>

<span class="hljs-keyword">val</span> allData: <span class="hljs-type">RDD</span>[(<span class="hljs-type">User</span>,<span class="hljs-type">TransactionCount</span>)] =
   sc.makeRDD((<span class="hljs-string">&quot;Bob&quot;</span>, <span class="hljs-number">12</span>) :: (<span class="hljs-string">&quot;Joe&quot;</span>, <span class="hljs-number">1</span>) :: (<span class="hljs-string">&quot;Anna&quot;</span>, <span class="hljs-number">100</span>) :: (<span class="hljs-string">&quot;Bob&quot;</span>, <span class="hljs-number">20</span>) :: (<span class="hljs-string">&quot;Joe&quot;</span>, <span class="hljs-number">2</span>) :: <span class="hljs-type">Nil</span>)
<span class="hljs-comment">// allData: org.apache.spark.rdd.RDD[(User, TransactionCount)] = ParallelCollectionRDD[16] at makeRDD at &lt;console&gt;:47</span>

<span class="hljs-keyword">val</span> totalPerUser =  allData.csumByKey
<span class="hljs-comment">// totalPerUser: org.apache.spark.rdd.RDD[(User, TransactionCount)] = ShuffledRDD[17] at reduceByKey at implicits.scala:42</span>

totalPerUser.collectAsMap
<span class="hljs-comment">// res16: scala.collection.Map[User,TransactionCount] = Map(Bob -&gt; 32, Joe -&gt; 3, Anna -&gt; 100)</span>
</code></pre>
<p>The same example would work for more complex keys.</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> scala.collection.immutable.<span class="hljs-type">SortedMap</span>
<span class="hljs-comment">// import scala.collection.immutable.SortedMap</span>

<span class="hljs-keyword">val</span> allDataComplexKeu =
   sc.makeRDD( (<span class="hljs-string">&quot;Bob&quot;</span>, <span class="hljs-type">SortedMap</span>(<span class="hljs-string">&quot;task1&quot;</span> -&gt; <span class="hljs-number">10</span>)) ::
    (<span class="hljs-string">&quot;Joe&quot;</span>, <span class="hljs-type">SortedMap</span>(<span class="hljs-string">&quot;task1&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;task2&quot;</span> -&gt; <span class="hljs-number">3</span>)) :: (<span class="hljs-string">&quot;Bob&quot;</span>, <span class="hljs-type">SortedMap</span>(<span class="hljs-string">&quot;task1&quot;</span> -&gt; <span class="hljs-number">10</span>, <span class="hljs-string">&quot;task2&quot;</span> -&gt; <span class="hljs-number">1</span>)) :: (<span class="hljs-string">&quot;Joe&quot;</span>, <span class="hljs-type">SortedMap</span>(<span class="hljs-string">&quot;task3&quot;</span> -&gt; <span class="hljs-number">4</span>)) :: <span class="hljs-type">Nil</span> )
<span class="hljs-comment">// allDataComplexKeu: org.apache.spark.rdd.RDD[(String, scala.collection.immutable.SortedMap[String,Int])] = ParallelCollectionRDD[18] at makeRDD at &lt;console&gt;:46</span>

<span class="hljs-keyword">val</span> overalTasksPerUser = allDataComplexKeu.csumByKey
<span class="hljs-comment">// overalTasksPerUser: org.apache.spark.rdd.RDD[(String, scala.collection.immutable.SortedMap[String,Int])] = ShuffledRDD[19] at reduceByKey at implicits.scala:42</span>

overalTasksPerUser.collectAsMap
<span class="hljs-comment">// res17: scala.collection.Map[String,scala.collection.immutable.SortedMap[String,Int]] = Map(Bob -&gt; Map(task1 -&gt; 20, task2 -&gt; 1), Joe -&gt; Map(task1 -&gt; 1, task2 -&gt; 3, task3 -&gt; 4))</span>
</code></pre>
<h4 id="joins">Joins</h4>
<pre><code class="lang-scala"><span class="hljs-comment">// Type aliases for meaningful types</span>
<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">TimeSeries</span> </span>= <span class="hljs-type">Map</span>[<span class="hljs-type">Int</span>,<span class="hljs-type">Int</span>]
<span class="hljs-comment">// defined type alias TimeSeries</span>

<span class="hljs-class"><span class="hljs-keyword">type</span> <span class="hljs-title">UserName</span> </span>= <span class="hljs-type">String</span>
<span class="hljs-comment">// defined type alias UserName</span>
</code></pre>
<p>Example: Using the implicit full-our-join operator</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> frameless.cats.outer._
<span class="hljs-comment">// import frameless.cats.outer._</span>

<span class="hljs-keyword">val</span> day1: <span class="hljs-type">RDD</span>[(<span class="hljs-type">UserName</span>,<span class="hljs-type">TimeSeries</span>)] = sc.makeRDD( (<span class="hljs-string">&quot;John&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">2</span>, <span class="hljs-number">1</span> -&gt; <span class="hljs-number">4</span>)) :: (<span class="hljs-string">&quot;Chris&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-number">1</span> -&gt; <span class="hljs-number">2</span>)) :: (<span class="hljs-string">&quot;Sam&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">1</span>)) :: <span class="hljs-type">Nil</span> )
<span class="hljs-comment">// day1: org.apache.spark.rdd.RDD[(UserName, TimeSeries)] = ParallelCollectionRDD[20] at makeRDD at &lt;console&gt;:50</span>

<span class="hljs-keyword">val</span> day2: <span class="hljs-type">RDD</span>[(<span class="hljs-type">UserName</span>,<span class="hljs-type">TimeSeries</span>)] = sc.makeRDD( (<span class="hljs-string">&quot;John&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">10</span>, <span class="hljs-number">1</span> -&gt; <span class="hljs-number">11</span>)) :: (<span class="hljs-string">&quot;Chris&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-number">1</span> -&gt; <span class="hljs-number">2</span>)) :: (<span class="hljs-string">&quot;Joe&quot;</span>, <span class="hljs-type">Map</span>(<span class="hljs-number">0</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-number">1</span> -&gt; <span class="hljs-number">2</span>)) :: <span class="hljs-type">Nil</span> )
<span class="hljs-comment">// day2: org.apache.spark.rdd.RDD[(UserName, TimeSeries)] = ParallelCollectionRDD[21] at makeRDD at &lt;console&gt;:50</span>

<span class="hljs-keyword">val</span> daysCombined = day1 |+| day2
<span class="hljs-comment">// daysCombined: org.apache.spark.rdd.RDD[(UserName, TimeSeries)] = MapPartitionsRDD[25] at mapValues at implicits.scala:67</span>

daysCombined.collect()
<span class="hljs-comment">// res19: Array[(UserName, TimeSeries)] = Array((Joe,Map(0 -&gt; 1, 1 -&gt; 2)), (Sam,Map(0 -&gt; 1)), (Chris,Map(0 -&gt; 2, 1 -&gt; 4)), (John,Map(0 -&gt; 12, 1 -&gt; 15)))</span>
</code></pre>
<p>Note how the user&apos;s timeseries from different days have been aggregated together.
The <code>|+|</code> (Semigroup) operator for key-value pair RDD will execute a full-outer-join
on the key and combine values using the default Semigroup for the value type.</p>
<p>In <code>cats</code>:</p>
<pre><code class="lang-scala"><span class="hljs-type">Map</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-number">2</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-number">3</span>) |+| <span class="hljs-type">Map</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-number">4</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-number">-1</span>)
<span class="hljs-comment">// res20: Map[Int,Int] = Map(1 -&gt; 6, 2 -&gt; 2)</span>
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Job.html" class="navigation navigation-prev " aria-label="Previous page: Job[A]">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="TypedML.html" class="navigation navigation-next " aria-label="Next page: Using Spark ML with TypedDataset">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Using Cats with RDDs","level":"1.7","depth":1,"next":{"title":"Using Spark ML with TypedDataset","level":"1.8","depth":1,"path":"TypedML.md","ref":"TypedML.md","articles":[]},"previous":{"title":"Job[A]","level":"1.6","depth":1,"path":"Job.md","ref":"Job.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":[],"pluginsConfig":{"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"Cats.md","mtime":"2018-02-18T23:18:30.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2018-02-18T23:19:28.928Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

