<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.18.1 + Helium Theme" />
    <title>Comparing TypedDatasets with Spark&#39;s Datasets</title>
    
      <meta name="author" content="Olivier Blanvillain"/>
    
      <meta name="author" content="Adelbert Chang"/>
    
      <meta name="author" content="Marios Iliofotou"/>
    
      <meta name="author" content="Gleb Kanterov"/>
    
      <meta name="author" content="Erik Osheim"/>
    
      <meta name="author" content="Jeremy Smith"/>
    
      <meta name="author" content="CÃ©dric Chantepie"/>
    
      <meta name="author" content="Grigory Pomadchin"/>
    
    
      <meta name="description" content="docs"/>
    
    
      <link rel="icon" sizes="32x32" type="image/png" href="https://typelevel.org/img/favicon.png"/>
    
    
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
    
    <link rel="stylesheet" type="text/css" href="helium/icofont.min.css" />
    <link rel="stylesheet" type="text/css" href="helium/laika-helium.css" />
    <link rel="stylesheet" type="text/css" href="site/styles.css" />
    <script src="helium/laika-helium.js"></script>
    
    
    <script> /* for avoiding page load transitions */ </script>
  </head>

  <body>

    <header id="top-bar">

      <div class="row">
        <a id="nav-icon">
          <i class="icofont-laika" title="Navigation">&#xefa2;</i>
        </a>
        
      </div>
  
      <a class="image-link" href="https://typelevel.org"><img src="https://typelevel.org/img/logo.svg"></a>
      
      <span class="row links"><a class="icon-link svg-link" href="https://github.com/typelevel/frameless"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a><a class="icon-link" href="https://discord.gg/XF3CXcMzqD"><i class="icofont-laika" title="Chat">&#xeed5;</i></a><a class="icon-link" href="https://twitter.com/typelevel"><i class="icofont-laika" title="Twitter">&#xed7a;</i></a></span>
      
    </header>

    <nav id="sidebar">

      <div class="row">
        <a class="icon-link svg-link" href="https://github.com/typelevel/frameless"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a><a class="icon-link" href="https://discord.gg/XF3CXcMzqD"><i class="icofont-laika" title="Chat">&#xeed5;</i></a><a class="icon-link" href="https://twitter.com/typelevel"><i class="icofont-laika" title="Twitter">&#xed7a;</i></a>
      </div>
      
      <ul class="nav-list">
        <li class="level1"><a href="FeatureOverview.html">TypedDataset: Feature Overview</a></li>
        <li class="level1 active"><a href="#">Comparing TypedDatasets with Spark&#39;s Datasets</a></li>
        <li class="level1"><a href="WorkingWithCsvParquetJson.html">Working with CSV and Parquet data</a></li>
        <li class="level1"><a href="Injection.html">Injection: Creating Custom Encoders</a></li>
        <li class="level1"><a href="Job.html">Job[A]</a></li>
        <li class="level1"><a href="Cats.html">Using Cats with Frameless</a></li>
        <li class="level1"><a href="TypedML.html">Typed Spark ML</a></li>
        <li class="level1"><a href="TypedDataFrame.html">Proof of Concept: TypedDataFrame</a></li>
        <li class="level1"><a href="TypedEncoder.html">Typed Encoders in Frameless</a></li>
      </ul>
      
      <ul class="nav-list">
        <li class="level1 nav-header">Related Projects</li>
        
          <li class="level2"><a href="https://typelevel.org/cats/">cats</a></li>
        
      </ul>

    </nav>

    <div id="container">

      <nav id="page-nav">
        <p class="header"><a href="#">Comparing TypedDatasets with Spark&#39;s Datasets</a></p>
        
        <ul class="nav-list">
          <li class="level1"><a href="#differences-in-encoders">Differences in Encoders</a></li>
          <li class="level1"><a href="#aggregate-vs-projected-columns">Aggregate vs Projected columns</a></li>
        </ul>
        
        <p class="footer"></p>
      </nav>

      <main class="content">

        <h1 id="comparing-typeddatasets-with-spark-s-datasets" class="title">Comparing TypedDatasets with Spark&#39;s Datasets</h1>
        <p><strong>Goal:</strong>
          This tutorial compares the standard Spark Datasets API with the one provided by
          Frameless&#39; <code>TypedDataset</code>. It shows how <code>TypedDataset</code>s allow for an expressive and
          type-safe api with no compromises on performance.</p>
        <p>For this tutorial we first create a simple dataset and save it on disk as a parquet file.
        <a href="https://parquet.apache.org/">Parquet</a> is a popular columnar format and well supported by Spark.
        It&#39;s important to note that when operating on parquet datasets, Spark knows that each column is stored
        separately, so if we only need a subset of the columns Spark will optimize for this and avoid reading
        the entire dataset. This is a rather simplistic view of how Spark and parquet work together but it
        will serve us well for the context of this discussion.</p>
        <pre><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">spark</span><span>.</span><span class="identifier">implicits</span><span>.</span><span class="identifier">_</span><span>

</span><span class="comment">// Our example case class Foo acting here as a schema
</span><span class="keyword">case</span><span> </span><span class="keyword">class</span><span> </span><span class="type-name">Foo</span><span>(</span><span class="identifier">i</span><span>: </span><span class="type-name">Long</span><span>, </span><span class="identifier">j</span><span>: </span><span class="type-name">String</span><span>)

</span><span class="comment">// Assuming spark is loaded and SparkSession is bind to spark
</span><span class="keyword">val</span><span> </span><span class="identifier">initialDs</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>( </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">1</span><span>, </span><span class="string-literal">&quot;Q&quot;</span><span>) :: </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">10</span><span>, </span><span class="string-literal">&quot;W&quot;</span><span>) :: </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">100</span><span>, </span><span class="string-literal">&quot;E&quot;</span><span>) :: </span><span class="type-name">Nil</span><span> )
</span><span class="comment">// initialDs: org.apache.spark.sql.Dataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="comment">// Assuming you are on Linux or Mac OS
</span><span class="identifier">initialDs</span><span>.</span><span class="identifier">write</span><span>.</span><span class="identifier">parquet</span><span>(</span><span class="string-literal">&quot;/tmp/foo&quot;</span><span>)

</span><span class="keyword">val</span><span> </span><span class="identifier">ds</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">read</span><span>.</span><span class="identifier">parquet</span><span>(</span><span class="string-literal">&quot;/tmp/foo&quot;</span><span>).</span><span class="identifier">as</span><span>[</span><span class="type-name">Foo</span><span>]
</span><span class="comment">// ds: org.apache.spark.sql.Dataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="identifier">ds</span><span>.</span><span class="identifier">show</span><span>()
</span><span class="comment">// +---+---+
// |  i|  j|
// +---+---+
// | 10|  W|
// |100|  E|
// |  1|  Q|
// +---+---+
//</span></code></pre>
        <p>The value <code>ds</code> holds the content of the <code>initialDs</code> read from a parquet file.
        Let&#39;s try to only use field <code>i</code> from Foo and see how Spark&#39;s Catalyst (the query optimizer)
        optimizes this.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// Using a standard Spark TypedColumn in select()
</span><span class="keyword">val</span><span> </span><span class="identifier">filteredDs</span><span> = </span><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span> === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>.</span><span class="identifier">as</span><span>[</span><span class="type-name">Long</span><span>])
</span><span class="comment">// filteredDs: org.apache.spark.sql.Dataset[Long] = [i: bigint]
</span><span>
</span><span class="identifier">filteredDs</span><span>.</span><span class="identifier">show</span><span>()
</span><span class="comment">// +---+
// |  i|
// +---+
// | 10|
// +---+
//</span></code></pre>
        <p>The <code>filteredDs</code> is of type <code>Dataset[Long]</code>. Since we only access field <code>i</code> from <code>Foo</code> the type is correct.
        Unfortunately, this syntax requires handholding by explicitly setting the <code>TypedColumn</code> in the <code>select</code> statement
        to return type <code>Long</code> (look at the <code>as[Long]</code> statement). We will discuss this limitation next in more detail.
        Now, let&#39;s take a quick look at the optimized Physical Plan that Spark&#39;s Catalyst generated.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">filteredDs</span><span>.</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) Filter (isnotnull(i#2063L) AND (i#2063L = 10))
// +- *(1) ColumnarToRow
//    +- FileScan parquet [i#2063L] Batched: true, DataFilters: [isnotnull(i#2063L), (i#2063L = 10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [IsNotNull(i), EqualTo(i,10)], ReadSchema: struct&lt;i:bigint&gt;
// 
//</span></code></pre>
        <p>The last line is very important (see <code>ReadSchema</code>). The schema read
        from the parquet file only required reading column <code>i</code> without needing to access column <code>j</code>.
        This is great! We have both an optimized query plan and type-safety!</p>
        <p>Unfortunately, this syntax is not bulletproof: it fails at run-time if we try to access
        a non existing column <code>x</code>:</p>
        <pre><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span> === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;x&quot;</span><span>.</span><span class="identifier">as</span><span>[</span><span class="type-name">Long</span><span>])
</span><span class="comment">// org.apache.spark.sql.AnalysisException: cannot resolve &#39;x&#39; given input columns: [i, j];
// &#39;Project [&#39;x]
// +- Filter (i#2063L = cast(10 as bigint))
//    +- Relation [i#2063L,j#2064] parquet
// 
// 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:179)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:175)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)
// 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)
// 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)
// 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
// 	at scala.collection.immutable.List.foreach(List.scala:431)
// 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
// 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
// 	at scala.collection.immutable.List.map(List.scala:305)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:175)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
// 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
// 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
// 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
// 	at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:205)
// 	at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:211)
// 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1509)
// 	at repl.MdocSession$App0$$anonfun$15.apply(TypedDatasetVsSparkDataset.md:78)
// 	at repl.MdocSession$App0$$anonfun$15.apply(TypedDatasetVsSparkDataset.md:78)</span></code></pre>
        <p>There are two things to improve here. First, we would want to avoid the <code>as[Long]</code> casting that we are required
        to type for type-safety. This is clearly an area where we may introduce a bug by casting to an incompatible
        type. Second, we want a solution where reference to a non existing column name fails at compilation time.
        The standard Spark Dataset can achieve this using the following syntax.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span>).</span><span class="identifier">show</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |   10|
// +-----+
//</span></code></pre>
        <p>This looks great! It reminds us the familiar syntax from Scala.
        The two closures in filter and map are functions that operate on <code>Foo</code> and the
        compiler will helps us capture all the mistakes we mentioned above.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">x</span><span>).</span><span class="identifier">show</span><span>()
</span><span class="comment">// error: value x is not a member of repl.MdocSession.App0.Foo
// ds.filter(_.i == 10).map(_.x).show()
//                          ^^^</span></code></pre>
        <p>Unfortunately, this syntax does not allow Spark to optimize the code.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span>).</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) SerializeFromObject [input[0, bigint, false] AS value#2114L]
// +- *(1) MapElements &lt;function1&gt;, obj#2113: bigint
//    +- *(1) Filter &lt;function1&gt;.apply
//       +- *(1) DeserializeToObject newInstance(class repl.MdocSession$App0$Foo), obj#2112: repl.MdocSession$App0$Foo
//          +- *(1) ColumnarToRow
//             +- FileScan parquet [i#2063L,j#2064] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;i:bigint,j:string&gt;
// 
//</span></code></pre>
        <p>As we see from the explained Physical Plan, Spark was not able to optimize our query as before.
        Reading the parquet file will required loading all the fields of <code>Foo</code>. This might be ok for
        small datasets or for datasets with few columns, but will be extremely slow for most practical
        applications. Intuitively, Spark currently does not have a way to look inside the code we pass in these two
        closures. It only knows that they both take one argument of type <code>Foo</code>, but it has no way of knowing if
        we use just one or all of <code>Foo</code>&#39;s fields.</p>
        <p>The <code>TypedDataset</code> in Frameless solves this problem. It allows for a simple and type-safe syntax
        with a fully optimized query plan.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="type-name">TypedDataset</span><span>
</span><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="identifier">syntax</span><span>.</span><span class="identifier">_</span><span>
</span><span class="keyword">val</span><span> </span><span class="identifier">fds</span><span> = </span><span class="type-name">TypedDataset</span><span>.</span><span class="identifier">create</span><span>(</span><span class="identifier">ds</span><span>)
</span><span class="comment">// fds: TypedDataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |   10|
// +-----+
//</span></code></pre>
        <p>And the optimized Physical Plan:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) Project [i#2063L AS value#2198L]
// +- *(1) Filter (isnotnull(i#2063L) AND (i#2063L = 10))
//    +- *(1) ColumnarToRow
//       +- FileScan parquet [i#2063L] Batched: true, DataFilters: [isnotnull(i#2063L), (i#2063L = 10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [IsNotNull(i), EqualTo(i,10)], ReadSchema: struct&lt;i:bigint&gt;
// 
//</span></code></pre>
        <p>And the compiler is our friend.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;x</span><span>))
</span><span class="comment">// error: No column Symbol with shapeless.tag.Tagged[String(&quot;x&quot;)] of type A in repl.MdocSession.App0.Foo
// fds.filter(fds(&#39;i) === 10).select(fds(&#39;x))
//                                      ^</span></code></pre>
        
        <h2 id="differences-in-encoders" class="section">Differences in Encoders<a class="anchor-link right" href="#differences-in-encoders"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Encoders in Spark&#39;s <code>Datasets</code> are partially type-safe. If you try to create a <code>Dataset</code> using  a type that is not 
         a Scala <code>Product</code> then you get a compilation error:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">class</span><span> </span><span class="type-name">Bar</span><span>(</span><span class="identifier">i</span><span>: </span><span class="type-name">Int</span><span>)</span></code></pre>
        <p><code>Bar</code> is neither a case class nor a <code>Product</code>, so the following correctly gives a compilation error in Spark:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="keyword">new</span><span> </span><span class="type-name">Bar</span><span>(</span><span class="number-literal">1</span><span>)))
</span><span class="comment">// error: Unable to find encoder for type repl.MdocSession.App0.Bar. An implicit Encoder[repl.MdocSession.App0.Bar] is needed to store repl.MdocSession.App0.Bar instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
// spark.createDataset(Seq(new Bar(1)))
// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span></code></pre>
        <p>However, the compile type guards implemented in Spark are not sufficient to detect non encodable members. 
        For example, using the following case class leads to a runtime failure:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">case</span><span> </span><span class="keyword">class</span><span> </span><span class="type-name">MyDate</span><span>(</span><span class="identifier">jday</span><span>: </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>)</span></code></pre>
        <pre><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">myDateDs</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="type-name">MyDate</span><span>(</span><span class="keyword">new</span><span> </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>(</span><span class="type-name">System</span><span>.</span><span class="identifier">currentTimeMillis</span><span>))))
</span><span class="comment">// java.lang.UnsupportedOperationException: No Encoder found for java.util.Date
// - field (class: &quot;java.util.Date&quot;, name: &quot;jday&quot;)
// - root class: &quot;repl.MdocSession.App0.MyDate&quot;
// 	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotFindEncoderForTypeError(QueryExecutionErrors.scala:1000)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:617)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:448)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$6(ScalaReflection.scala:604)
// 	at scala.collection.immutable.List.map(List.scala:293)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:589)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:448)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:437)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:429)
// 	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55)
// 	at org.apache.spark.sql.Encoders$.product(Encoders.scala:300)
// 	at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder(SQLImplicits.scala:261)
// 	at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder$(SQLImplicits.scala:261)
// 	at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:32)
// 	at repl.MdocSession$App0$$anonfun$41.apply$mcV$sp(TypedDatasetVsSparkDataset.md:151)
// 	at repl.MdocSession$App0$$anonfun$41.apply(TypedDatasetVsSparkDataset.md:150)
// 	at repl.MdocSession$App0$$anonfun$41.apply(TypedDatasetVsSparkDataset.md:150)</span></code></pre>
        <p>In comparison, a TypedDataset will notify about the encoding problem at compile time: </p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="type-name">TypedDataset</span><span>.</span><span class="identifier">create</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="type-name">MyDate</span><span>(</span><span class="keyword">new</span><span> </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>(</span><span class="type-name">System</span><span>.</span><span class="identifier">currentTimeMillis</span><span>))))
</span><span class="comment">// error: could not find implicit value for parameter encoder: frameless.TypedEncoder[repl.MdocSession.App0.MyDate]
// TypedDataset.create(Seq(MyDate(new java.util.Date(System.currentTimeMillis))))
// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span></code></pre>
        
        <h2 id="aggregate-vs-projected-columns" class="section">Aggregate vs Projected columns<a class="anchor-link right" href="#aggregate-vs-projected-columns"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Spark&#39;s <code>Dataset</code> do not distinguish between columns created from aggregate operations, 
        such as summing or averaging, and simple projections/selections. 
        This is problematic when you start mixing the two.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">org</span><span>.</span><span class="identifier">apache</span><span>.</span><span class="identifier">spark</span><span>.</span><span class="identifier">sql</span><span>.</span><span class="identifier">functions</span><span>.</span><span class="identifier">sum</span></code></pre>
        <pre><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">select</span><span>(</span><span class="identifier">sum</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>), </span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>*</span><span class="number-literal">2</span><span>)
</span><span class="comment">// org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and &#39;i&#39; is not an aggregate function. Wrap &#39;(sum(i) AS `sum(i)`)&#39; in windowing function(s) or wrap &#39;i&#39; in first() (or first_value) if you don&#39;t care which value you get.;
// Aggregate [sum(i#2063L) AS sum(i)#2204L, (i#2063L * cast(2 as bigint)) AS (i * 2)#2205L]
// +- Relation [i#2063L,j#2064] parquet
// 
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:51)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:50)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:296)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:311)
// 	at scala.collection.Iterator.foreach(Iterator.scala:943)
// 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
// 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
// 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
// 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
// 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:311)
// 	at scala.collection.Iterator.foreach(Iterator.scala:943)
// 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
// 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
// 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
// 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
// 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$16(CheckAnalysis.scala:338)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$16$adapted(CheckAnalysis.scala:338)
// 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
// 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
// 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:338)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
// 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
// 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
// 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
// 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
// 	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3734)
// 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1454)
// 	at repl.MdocSession$App0$$anonfun$44.apply(TypedDatasetVsSparkDataset.md:173)
// 	at repl.MdocSession$App0$$anonfun$44.apply(TypedDatasetVsSparkDataset.md:173)</span></code></pre>
        <p>In Frameless, mixing the two results in a compilation error.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// To avoid confusing frameless&#39; sum with the standard Spark&#39;s sum
</span><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="identifier">functions</span><span>.</span><span class="identifier">aggregate</span><span>.{</span><span class="identifier">sum</span><span> =&gt; </span><span class="identifier">fsum</span><span>}</span></code></pre>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">select</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)))
</span><span class="comment">// error: polymorphic expression cannot be instantiated to expected type;
//  found   : [Out]frameless.TypedAggregate[repl.MdocSession.App0.Foo,Out]
//  required: frameless.TypedColumn[repl.MdocSession.App0.Foo,?]
// fds.select(fsum(fds(&#39;i)))
//            ^^^^^^^^^^^^^</span></code></pre>
        <p>As the error suggests, we expected a <code>TypedColumn</code> but we got a <code>TypedAggregate</code> instead. </p>
        <p>Here is how you apply an aggregation method in Frameless: </p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">agg</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>))+</span><span class="number-literal">22</span><span>).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |  133|
// +-----+
//</span></code></pre>
        <p>Similarly, mixing projections while aggregating does not make sense, and in Frameless
        you get a compilation error.<br></p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">agg</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)), </span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// error: polymorphic expression cannot be instantiated to expected type;
//  found   : [A]frameless.TypedColumn[repl.MdocSession.App0.Foo,A]
//  required: frameless.TypedAggregate[repl.MdocSession.App0.Foo,?]
// fds.agg(fsum(fds(&#39;i)), fds(&#39;i)).show().run()
//                        ^^^^^^^</span></code></pre>

        <hr style="margin-top: 30px"/>
        <footer style="font-size: 90%; text-align: center">
          frameless is a <a href="https://typelevel.org/">Typelevel</a> project distributed under the <a href="http://opensource.org/licenses/Apache-2.0">Apache-2.0</a> license.
        </footer>

      </main>

    </div>

  </body>
</html>
