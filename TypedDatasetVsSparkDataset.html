<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Laika 0.18.1 + Helium Theme" />
    <title>Comparing TypedDatasets with Spark&#39;s Datasets</title>
    
    
      <meta name="description" content="docs"/>
    
    
    
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
    
    <link rel="stylesheet" type="text/css" href="helium/icofont.min.css" />
    <link rel="stylesheet" type="text/css" href="helium/laika-helium.css" />
    <script src="helium/laika-helium.js"></script>
    
    
    <script> /* for avoiding page load transitions */ </script>
  </head>

  <body>

    <header id="top-bar">

      <div class="row">
        <a id="nav-icon">
          <i class="icofont-laika" title="Navigation">&#xefa2;</i>
        </a>
        
      </div>
  
      <a class="image-link" href="https://typelevel.org"><img src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>

<svg
    width="120px"
    height="40px"
    viewBox="0 0 1054 348"
    version="1.1"
    xmlns="http://www.w3.org/2000/svg"
    xmlns:xlink="http://www.w3.org/1999/xlink">

    <defs>
        <linearGradient x1="0%" y1="61%" x2="100%" y2="37%" id="background-gradient">
            <stop stop-color="#FF4C61" offset="0%"></stop>
            <stop stop-color="#F51C2B" offset="100%"></stop>
        </linearGradient>
    </defs>

    <g>
        <path d="M419.765714,55.1106383 L419.765714,133.12766 L398.525714,133.12766 L398.525714,55.1106383 L381.078571,55.1106383 L381.078571,36.9319149 L437.971429,36.9319149 L437.971429,55.1106383 L419.765714,55.1106383 L419.765714,55.1106383 Z M495.622857,98.2851064 L495.622857,133.12766 L474.382857,133.12766 L474.382857,98.2851064 L450.108571,36.1744681 L473.624286,36.1744681 L485.002857,76.3191489 L496.381429,36.1744681 L519.897143,36.1744681 L495.622857,98.2851064 L495.622857,98.2851064 Z M600.305714,67.2297872 C600.305714,72.5319149 599.547143,77.0765957 598.03,80.8638298 C596.512857,84.6510638 594.237143,87.6808511 591.202857,90.7106383 C588.168571,93.7404255 585.134286,95.2553191 582.1,96.012766 C579.065714,96.7702128 575.272857,97.5276596 571.48,97.5276596 L562.377143,97.5276596 L562.377143,132.370213 L541.137143,132.370213 L541.137143,36.9319149 L571.48,36.9319149 C575.272857,36.9319149 578.307143,37.6893617 582.1,38.4468085 C585.892857,39.2042553 588.927143,41.4765957 591.961429,43.7489362 C594.995714,46.0212766 597.271429,49.0510638 598.788571,52.8382979 C599.547143,56.6255319 600.305714,61.1702128 600.305714,67.2297872 L600.305714,67.2297872 Z M579.824286,67.2297872 C579.824286,62.6851064 579.065714,59.6553191 576.79,58.1404255 C574.514286,55.8680851 572.238571,55.1106383 569.204286,55.1106383 L562.377143,55.1106383 L562.377143,80.106383 L569.204286,80.106383 C572.238571,80.106383 574.514286,79.3489362 576.79,77.0765957 C579.065714,74.8042553 579.824286,71.7744681 579.824286,67.2297872 L579.824286,67.2297872 Z M622.304286,133.12766 L622.304286,36.9319149 L669.335714,36.9319149 L669.335714,55.8680851 L642.785714,55.8680851 L642.785714,74.8042553 L666.301429,74.8042553 L666.301429,92.9829787 L642.785714,92.9829787 L642.785714,113.434043 L670.094286,113.434043 L670.094286,133.12766 L622.304286,133.12766 L622.304286,133.12766 Z M696.644286,133.12766 L696.644286,36.9319149 L718.642857,36.9319149 L718.642857,113.434043 L744.434286,113.434043 L744.434286,133.885106 L696.644286,133.885106 L696.644286,133.12766 Z M767.95,133.12766 L767.95,36.9319149 L814.981429,36.9319149 L814.981429,55.8680851 L789.19,55.8680851 L789.19,74.8042553 L812.705714,74.8042553 L812.705714,92.9829787 L789.19,92.9829787 L789.19,113.434043 L816.498571,113.434043 L816.498571,133.12766 L767.95,133.12766 L767.95,133.12766 Z M885.528571,133.12766 L857.461429,133.12766 L833.945714,36.9319149 L858.978571,36.9319149 L871.874286,107.374468 L884.77,36.9319149 L908.285714,36.9319149 L885.528571,133.12766 L885.528571,133.12766 Z M931.042857,133.12766 L931.042857,36.9319149 L978.074286,36.9319149 L978.074286,55.8680851 L952.282857,55.8680851 L952.282857,74.8042553 L975.798571,74.8042553 L975.798571,92.9829787 L952.282857,92.9829787 L952.282857,113.434043 L979.591429,113.434043 L979.591429,133.12766 L931.042857,133.12766 L931.042857,133.12766 Z M1006.14143,133.12766 L1006.14143,36.9319149 L1028.14,36.9319149 L1028.14,113.434043 L1053.93143,113.434043 L1053.93143,133.885106 L1006.14143,133.885106 L1006.14143,133.12766 Z M429.627143,233.868085 C428.11,231.595745 425.834286,230.838298 423.558571,229.323404 C421.282857,228.565957 419.007143,227.808511 416.731429,227.808511 C414.455714,227.808511 412.18,228.565957 409.904286,230.080851 C407.628571,231.595745 406.87,233.868085 406.87,237.655319 C406.87,240.685106 407.628571,242.957447 409.904286,244.47234 C412.18,245.987234 415.214286,248.259574 419.007143,249.774468 C421.282857,250.531915 423.558571,252.046809 425.834286,253.561702 C428.11,255.076596 430.385714,256.591489 432.661429,258.86383 C434.937143,261.13617 436.454286,263.408511 437.971429,266.438298 C439.488571,269.468085 440.247143,273.255319 440.247143,277.042553 C440.247143,282.344681 439.488571,286.889362 437.971429,290.676596 C436.454286,294.46383 434.178571,298.251064 431.144286,300.523404 C428.11,302.795745 425.075714,305.068085 421.282857,306.582979 C417.49,308.097872 413.697143,308.855319 409.904286,308.855319 C403.835714,308.855319 398.525714,308.097872 393.974286,305.825532 C389.422857,303.553191 385.63,301.280851 382.595714,298.251064 L393.974286,281.587234 C396.25,283.859574 398.525714,285.374468 401.56,286.889362 C404.594286,288.404255 406.87,289.161702 409.904286,289.161702 C412.18,289.161702 414.455714,288.404255 416.731429,286.889362 C418.248571,285.374468 419.765714,283.102128 419.765714,279.314894 C419.765714,276.285106 419.007143,273.255319 416.731429,271.740426 C414.455714,270.225532 411.421429,267.953191 406.87,265.680851 L399.284286,261.13617 C397.008571,259.621277 394.732857,258.106383 393.215714,255.834043 C391.698571,253.561702 390.181429,251.289362 389.422857,248.259574 C388.664286,245.229787 387.905714,242.2 387.905714,237.655319 C387.905714,232.353191 388.664286,227.808511 390.94,224.021277 C392.457143,220.234043 394.732857,217.204255 397.767143,214.174468 C400.801429,211.902128 403.835714,209.629787 407.628571,208.87234 C411.421429,207.357447 414.455714,207.357447 418.248571,207.357447 C423.558571,207.357447 428.11,208.114894 432.661429,209.629787 C437.212857,211.144681 440.247143,213.417021 443.281429,216.446809 L429.627143,233.868085 L429.627143,233.868085 Z M511.552857,305.825532 C507.001429,308.097872 501.691429,309.612766 494.864286,309.612766 C489.554286,309.612766 484.244286,308.097872 479.692857,305.825532 C475.141429,303.553191 471.348571,299.765957 467.555714,295.221277 C463.762857,290.676596 461.487143,285.374468 459.97,279.314894 C458.452857,273.255319 456.935714,266.438298 456.935714,258.86383 C456.935714,251.289362 457.694286,244.47234 459.97,238.412766 C461.487143,232.353191 464.521429,227.051064 467.555714,222.506383 C470.59,217.961702 475.141429,214.931915 479.692857,211.902128 C484.244286,209.629787 489.554286,208.114894 494.864286,208.114894 C501.691429,208.114894 507.001429,209.629787 510.794286,211.902128 C514.587143,214.174468 518.38,217.204255 521.414286,220.234043 L510.035714,236.140426 C508.518571,233.868085 506.242857,232.353191 504.725714,230.838298 C503.208571,229.323404 500.174286,228.565957 497.14,228.565957 C494.105714,228.565957 491.83,229.323404 489.554286,230.838298 C487.278571,232.353191 485.761429,234.625532 484.244286,236.897872 C482.727143,239.92766 481.968571,242.957447 481.21,246.744681 C480.451429,250.531915 480.451429,254.319149 480.451429,258.86383 C480.451429,263.408511 480.451429,267.195745 481.21,270.225532 C481.968571,274.012766 482.727143,277.042553 484.244286,280.07234 C485.761429,283.102128 487.278571,284.617021 489.554286,286.131915 C491.83,287.646809 494.105714,288.404255 497.14,288.404255 C500.174286,288.404255 502.45,287.646809 504.725714,286.131915 C507.001429,284.617021 508.518571,283.102128 510.035714,280.829787 L522.172857,295.978723 C519.897143,300.523404 516.104286,303.553191 511.552857,305.825532 L511.552857,305.825532 Z M585.892857,307.340426 L582.1,289.919149 L560.101429,289.919149 L556.308571,307.340426 L534.31,307.340426 L557.825714,211.144681 L585.134286,211.144681 L608.65,307.340426 L585.892857,307.340426 L585.892857,307.340426 Z M571.48,231.595745 L562.377143,272.497872 L579.065714,272.497872 L571.48,231.595745 L571.48,231.595745 Z M630.648571,307.340426 L630.648571,211.144681 L652.647143,211.144681 L652.647143,287.646809 L678.438571,287.646809 L678.438571,308.097872 L630.648571,308.097872 L630.648571,307.340426 Z M745.192857,307.340426 L741.4,289.919149 L719.401429,289.919149 L715.608571,307.340426 L693.61,307.340426 L717.125714,211.144681 L744.434286,211.144681 L767.95,307.340426 L745.192857,307.340426 L745.192857,307.340426 Z M730.021429,231.595745 L720.918571,272.497872 L737.607143,272.497872 L730.021429,231.595745 L730.021429,231.595745 Z" fill="#21303F" />

        <g id="logo">
            <polygon fill="url(#background-gradient)" points="151 348 0 260 0 88 151 0 302 88 302 260" />
            <polygon fill="#FF6169" points="99 214 99 254 193 199 193 159" />
            <polygon fill="#FF6169" points="134 194 99 174 193 119 228 139" />
            <polygon fill="#FF6169" points="99 134 64 114 158 59 193 79" />
            <polygon fill="#FFB4B5" points="134 194 134 234 228 179 228 139" />
            <polygon fill="#FFB4B5" points="99 134 99 174 193 119 193 79" />
            <polygon fill="#FFFFFF" points="64 234 99 254 99 214 134 234 134 194 99 174 99 134 64 114" />
        </g>
    </g>
</svg>
"></a>
      
      <span class="row links"><a class="icon-link svg-link" href="https://github.com/typelevel/frameless"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a><a class="icon-link" href="https://discord.gg/XF3CXcMzqD"><i class="icofont-laika" title="Chat">&#xeed5;</i></a><a class="icon-link" href="https://twitter.com/typelevel"><i class="icofont-laika" title="Twitter">&#xed7a;</i></a></span>
      
    </header>

    <nav id="sidebar">

      <div class="row">
        <a class="icon-link svg-link" href="https://github.com/typelevel/frameless"><span title="Source Code"><svg class="svg-icon" width="100%" height="100%" viewBox="0 0 100 100" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
  <g class="svg-shape">
    <path d="M49.995,1c-27.609,-0 -49.995,22.386 -49.995,50.002c-0,22.09 14.325,40.83 34.194,47.444c2.501,0.458 3.413,-1.086 3.413,-2.412c0,-1.185 -0.043,-4.331 -0.067,-8.503c-13.908,3.021 -16.843,-6.704 -16.843,-6.704c-2.274,-5.773 -5.552,-7.311 -5.552,-7.311c-4.54,-3.103 0.344,-3.042 0.344,-3.042c5.018,0.356 7.658,5.154 7.658,5.154c4.46,7.64 11.704,5.433 14.552,4.156c0.454,-3.232 1.744,-5.436 3.174,-6.685c-11.102,-1.262 -22.775,-5.553 -22.775,-24.713c-0,-5.457 1.949,-9.92 5.147,-13.416c-0.516,-1.265 -2.231,-6.348 0.488,-13.233c0,0 4.199,-1.344 13.751,5.126c3.988,-1.108 8.266,-1.663 12.518,-1.682c4.245,0.019 8.523,0.574 12.517,1.682c9.546,-6.47 13.736,-5.126 13.736,-5.126c2.728,6.885 1.013,11.968 0.497,13.233c3.204,3.496 5.141,7.959 5.141,13.416c0,19.209 -11.691,23.436 -22.83,24.673c1.795,1.544 3.394,4.595 3.394,9.26c0,6.682 -0.061,12.076 -0.061,13.715c0,1.338 0.899,2.894 3.438,2.406c19.853,-6.627 34.166,-25.354 34.166,-47.438c-0,-27.616 -22.389,-50.002 -50.005,-50.002"/>
  </g>
</svg></span></a><a class="icon-link" href="https://discord.gg/XF3CXcMzqD"><i class="icofont-laika" title="Chat">&#xeed5;</i></a><a class="icon-link" href="https://twitter.com/typelevel"><i class="icofont-laika" title="Twitter">&#xed7a;</i></a>
      </div>
      
      <ul class="nav-list">
        <li class="level1"><a href="FeatureOverview.html">TypedDataset: Feature Overview</a></li>
        <li class="level1 active"><a href="#">Comparing TypedDatasets with Spark&#39;s Datasets</a></li>
        <li class="level1"><a href="WorkingWithCsvParquetJson.html">Working with CSV and Parquet data</a></li>
        <li class="level1"><a href="Injection.html">Injection: Creating Custom Encoders</a></li>
        <li class="level1"><a href="Job.html">Job[A]</a></li>
        <li class="level1"><a href="Cats.html">Using Cats with Frameless</a></li>
        <li class="level1"><a href="TypedML.html">Typed Spark ML</a></li>
        <li class="level1"><a href="TypedDataFrame.html">Proof of Concept: TypedDataFrame</a></li>
        <li class="level1"><a href="TypedEncoder.html">Typed Encoders in Frameless</a></li>
      </ul>
      
    </nav>

    <div id="container">

      <nav id="page-nav">
        <p class="header"><a href="#">Comparing TypedDatasets with Spark&#39;s Datasets</a></p>
        
        <ul class="nav-list">
          <li class="level1"><a href="#differences-in-encoders">Differences in Encoders</a></li>
          <li class="level1"><a href="#aggregate-vs-projected-columns">Aggregate vs Projected columns</a></li>
        </ul>
        
        <p class="footer"></p>
      </nav>

      <main class="content">

        <h1 id="comparing-typeddatasets-with-spark-s-datasets" class="title">Comparing TypedDatasets with Spark&#39;s Datasets</h1>
        <p><strong>Goal:</strong>
          This tutorial compares the standard Spark Datasets API with the one provided by
          Frameless&#39; <code>TypedDataset</code>. It shows how <code>TypedDataset</code>s allow for an expressive and
          type-safe api with no compromises on performance.</p>
        <p>For this tutorial we first create a simple dataset and save it on disk as a parquet file.
        <a href="https://parquet.apache.org/">Parquet</a> is a popular columnar format and well supported by Spark.
        It&#39;s important to note that when operating on parquet datasets, Spark knows that each column is stored
        separately, so if we only need a subset of the columns Spark will optimize for this and avoid reading
        the entire dataset. This is a rather simplistic view of how Spark and parquet work together but it
        will serve us well for the context of this discussion.</p>
        <pre><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">spark</span><span>.</span><span class="identifier">implicits</span><span>.</span><span class="identifier">_</span><span>

</span><span class="comment">// Our example case class Foo acting here as a schema
</span><span class="keyword">case</span><span> </span><span class="keyword">class</span><span> </span><span class="type-name">Foo</span><span>(</span><span class="identifier">i</span><span>: </span><span class="type-name">Long</span><span>, </span><span class="identifier">j</span><span>: </span><span class="type-name">String</span><span>)

</span><span class="comment">// Assuming spark is loaded and SparkSession is bind to spark
</span><span class="keyword">val</span><span> </span><span class="identifier">initialDs</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>( </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">1</span><span>, </span><span class="string-literal">&quot;Q&quot;</span><span>) :: </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">10</span><span>, </span><span class="string-literal">&quot;W&quot;</span><span>) :: </span><span class="type-name">Foo</span><span>(</span><span class="number-literal">100</span><span>, </span><span class="string-literal">&quot;E&quot;</span><span>) :: </span><span class="type-name">Nil</span><span> )
</span><span class="comment">// initialDs: org.apache.spark.sql.Dataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="comment">// Assuming you are on Linux or Mac OS
</span><span class="identifier">initialDs</span><span>.</span><span class="identifier">write</span><span>.</span><span class="identifier">parquet</span><span>(</span><span class="string-literal">&quot;/tmp/foo&quot;</span><span>)

</span><span class="keyword">val</span><span> </span><span class="identifier">ds</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">read</span><span>.</span><span class="identifier">parquet</span><span>(</span><span class="string-literal">&quot;/tmp/foo&quot;</span><span>).</span><span class="identifier">as</span><span>[</span><span class="type-name">Foo</span><span>]
</span><span class="comment">// ds: org.apache.spark.sql.Dataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="identifier">ds</span><span>.</span><span class="identifier">show</span><span>()
</span><span class="comment">// +---+---+
// |  i|  j|
// +---+---+
// | 10|  W|
// |100|  E|
// |  1|  Q|
// +---+---+
//</span></code></pre>
        <p>The value <code>ds</code> holds the content of the <code>initialDs</code> read from a parquet file.
        Let&#39;s try to only use field <code>i</code> from Foo and see how Spark&#39;s Catalyst (the query optimizer)
        optimizes this.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// Using a standard Spark TypedColumn in select()
</span><span class="keyword">val</span><span> </span><span class="identifier">filteredDs</span><span> = </span><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span> === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>.</span><span class="identifier">as</span><span>[</span><span class="type-name">Long</span><span>])
</span><span class="comment">// filteredDs: org.apache.spark.sql.Dataset[Long] = [i: bigint]
</span><span>
</span><span class="identifier">filteredDs</span><span>.</span><span class="identifier">show</span><span>()
</span><span class="comment">// +---+
// |  i|
// +---+
// | 10|
// +---+
//</span></code></pre>
        <p>The <code>filteredDs</code> is of type <code>Dataset[Long]</code>. Since we only access field <code>i</code> from <code>Foo</code> the type is correct.
        Unfortunately, this syntax requires handholding by explicitly setting the <code>TypedColumn</code> in the <code>select</code> statement
        to return type <code>Long</code> (look at the <code>as[Long]</code> statement). We will discuss this limitation next in more detail.
        Now, let&#39;s take a quick look at the optimized Physical Plan that Spark&#39;s Catalyst generated.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">filteredDs</span><span>.</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) Filter (isnotnull(i#2063L) AND (i#2063L = 10))
// +- *(1) ColumnarToRow
//    +- FileScan parquet [i#2063L] Batched: true, DataFilters: [isnotnull(i#2063L), (i#2063L = 10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [IsNotNull(i), EqualTo(i,10)], ReadSchema: struct&lt;i:bigint&gt;
// 
//</span></code></pre>
        <p>The last line is very important (see <code>ReadSchema</code>). The schema read
        from the parquet file only required reading column <code>i</code> without needing to access column <code>j</code>.
        This is great! We have both an optimized query plan and type-safety!</p>
        <p>Unfortunately, this syntax is not bulletproof: it fails at run-time if we try to access
        a non existing column <code>x</code>:</p>
        <pre><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span> === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;x&quot;</span><span>.</span><span class="identifier">as</span><span>[</span><span class="type-name">Long</span><span>])
</span><span class="comment">// org.apache.spark.sql.AnalysisException: cannot resolve &#39;x&#39; given input columns: [i, j];
// &#39;Project [&#39;x]
// +- Filter (i#2063L = cast(10 as bigint))
//    +- Relation [i#2063L,j#2064] parquet
// 
// 	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:179)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:175)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:535)
// 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:535)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:181)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)
// 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:209)
// 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
// 	at scala.collection.immutable.List.foreach(List.scala:431)
// 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
// 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
// 	at scala.collection.immutable.List.map(List.scala:305)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:209)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:181)
// 	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:161)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:175)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
// 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
// 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
// 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
// 	at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:205)
// 	at org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:211)
// 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1509)
// 	at repl.MdocSession$App0$$anonfun$15.apply(TypedDatasetVsSparkDataset.md:78)
// 	at repl.MdocSession$App0$$anonfun$15.apply(TypedDatasetVsSparkDataset.md:78)</span></code></pre>
        <p>There are two things to improve here. First, we would want to avoid the <code>as[Long]</code> casting that we are required
        to type for type-safety. This is clearly an area where we may introduce a bug by casting to an incompatible
        type. Second, we want a solution where reference to a non existing column name fails at compilation time.
        The standard Spark Dataset can achieve this using the following syntax.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span>).</span><span class="identifier">show</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |   10|
// +-----+
//</span></code></pre>
        <p>This looks great! It reminds us the familiar syntax from Scala.
        The two closures in filter and map are functions that operate on <code>Foo</code> and the
        compiler will helps us capture all the mistakes we mentioned above.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">x</span><span>).</span><span class="identifier">show</span><span>()
</span><span class="comment">// error: value x is not a member of repl.MdocSession.App0.Foo
// ds.filter(_.i == 10).map(_.x).show()
//                          ^^^</span></code></pre>
        <p>Unfortunately, this syntax does not allow Spark to optimize the code.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span> == </span><span class="number-literal">10</span><span>).</span><span class="identifier">map</span><span>(</span><span class="identifier">_</span><span>.</span><span class="identifier">i</span><span>).</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) SerializeFromObject [input[0, bigint, false] AS value#2114L]
// +- *(1) MapElements &lt;function1&gt;, obj#2113: bigint
//    +- *(1) Filter &lt;function1&gt;.apply
//       +- *(1) DeserializeToObject newInstance(class repl.MdocSession$App0$Foo), obj#2112: repl.MdocSession$App0$Foo
//          +- *(1) ColumnarToRow
//             +- FileScan parquet [i#2063L,j#2064] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;i:bigint,j:string&gt;
// 
//</span></code></pre>
        <p>As we see from the explained Physical Plan, Spark was not able to optimize our query as before.
        Reading the parquet file will required loading all the fields of <code>Foo</code>. This might be ok for
        small datasets or for datasets with few columns, but will be extremely slow for most practical
        applications. Intuitively, Spark currently does not have a way to look inside the code we pass in these two
        closures. It only knows that they both take one argument of type <code>Foo</code>, but it has no way of knowing if
        we use just one or all of <code>Foo</code>&#39;s fields.</p>
        <p>The <code>TypedDataset</code> in Frameless solves this problem. It allows for a simple and type-safe syntax
        with a fully optimized query plan.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="type-name">TypedDataset</span><span>
</span><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="identifier">syntax</span><span>.</span><span class="identifier">_</span><span>
</span><span class="keyword">val</span><span> </span><span class="identifier">fds</span><span> = </span><span class="type-name">TypedDataset</span><span>.</span><span class="identifier">create</span><span>(</span><span class="identifier">ds</span><span>)
</span><span class="comment">// fds: TypedDataset[Foo] = [i: bigint, j: string]
</span><span>
</span><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |   10|
// +-----+
//</span></code></pre>
        <p>And the optimized Physical Plan:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">explain</span><span>()
</span><span class="comment">// == Physical Plan ==
// *(1) Project [i#2063L AS value#2198L]
// +- *(1) Filter (isnotnull(i#2063L) AND (i#2063L = 10))
//    +- *(1) ColumnarToRow
//       +- FileScan parquet [i#2063L] Batched: true, DataFilters: [isnotnull(i#2063L), (i#2063L = 10)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/foo], PartitionFilters: [], PushedFilters: [IsNotNull(i), EqualTo(i,10)], ReadSchema: struct&lt;i:bigint&gt;
// 
//</span></code></pre>
        <p>And the compiler is our friend.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">filter</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>) === </span><span class="number-literal">10</span><span>).</span><span class="identifier">select</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;x</span><span>))
</span><span class="comment">// error: No column Symbol with shapeless.tag.Tagged[String(&quot;x&quot;)] of type A in repl.MdocSession.App0.Foo
// fds.filter(fds(&#39;i) === 10).select(fds(&#39;x))
//                                      ^</span></code></pre>
        
        <h2 id="differences-in-encoders" class="section">Differences in Encoders<a class="anchor-link right" href="#differences-in-encoders"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Encoders in Spark&#39;s <code>Datasets</code> are partially type-safe. If you try to create a <code>Dataset</code> using  a type that is not 
         a Scala <code>Product</code> then you get a compilation error:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">class</span><span> </span><span class="type-name">Bar</span><span>(</span><span class="identifier">i</span><span>: </span><span class="type-name">Int</span><span>)</span></code></pre>
        <p><code>Bar</code> is neither a case class nor a <code>Product</code>, so the following correctly gives a compilation error in Spark:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="keyword">new</span><span> </span><span class="type-name">Bar</span><span>(</span><span class="number-literal">1</span><span>)))
</span><span class="comment">// error: Unable to find encoder for type repl.MdocSession.App0.Bar. An implicit Encoder[repl.MdocSession.App0.Bar] is needed to store repl.MdocSession.App0.Bar instances in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.
// spark.createDataset(Seq(new Bar(1)))
// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span></code></pre>
        <p>However, the compile type guards implemented in Spark are not sufficient to detect non encodable members. 
        For example, using the following case class leads to a runtime failure:</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">case</span><span> </span><span class="keyword">class</span><span> </span><span class="type-name">MyDate</span><span>(</span><span class="identifier">jday</span><span>: </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>)</span></code></pre>
        <pre><code class="nohighlight"><span class="keyword">val</span><span> </span><span class="identifier">myDateDs</span><span> = </span><span class="identifier">spark</span><span>.</span><span class="identifier">createDataset</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="type-name">MyDate</span><span>(</span><span class="keyword">new</span><span> </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>(</span><span class="type-name">System</span><span>.</span><span class="identifier">currentTimeMillis</span><span>))))
</span><span class="comment">// java.lang.UnsupportedOperationException: No Encoder found for java.util.Date
// - field (class: &quot;java.util.Date&quot;, name: &quot;jday&quot;)
// - root class: &quot;repl.MdocSession.App0.MyDate&quot;
// 	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotFindEncoderForTypeError(QueryExecutionErrors.scala:1000)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:617)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:448)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$6(ScalaReflection.scala:604)
// 	at scala.collection.immutable.List.map(List.scala:293)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:589)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:448)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:437)
// 	at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:73)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:947)
// 	at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:946)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:51)
// 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:429)
// 	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55)
// 	at org.apache.spark.sql.Encoders$.product(Encoders.scala:300)
// 	at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder(SQLImplicits.scala:261)
// 	at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder$(SQLImplicits.scala:261)
// 	at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:32)
// 	at repl.MdocSession$App0$$anonfun$41.apply$mcV$sp(TypedDatasetVsSparkDataset.md:151)
// 	at repl.MdocSession$App0$$anonfun$41.apply(TypedDatasetVsSparkDataset.md:150)
// 	at repl.MdocSession$App0$$anonfun$41.apply(TypedDatasetVsSparkDataset.md:150)</span></code></pre>
        <p>In comparison, a TypedDataset will notify about the encoding problem at compile time: </p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="type-name">TypedDataset</span><span>.</span><span class="identifier">create</span><span>(</span><span class="type-name">Seq</span><span>(</span><span class="type-name">MyDate</span><span>(</span><span class="keyword">new</span><span> </span><span class="identifier">java</span><span>.</span><span class="identifier">util</span><span>.</span><span class="type-name">Date</span><span>(</span><span class="type-name">System</span><span>.</span><span class="identifier">currentTimeMillis</span><span>))))
</span><span class="comment">// error: could not find implicit value for parameter encoder: frameless.TypedEncoder[repl.MdocSession.App0.MyDate]
// TypedDataset.create(Seq(MyDate(new java.util.Date(System.currentTimeMillis))))
// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span></code></pre>
        
        <h2 id="aggregate-vs-projected-columns" class="section">Aggregate vs Projected columns<a class="anchor-link right" href="#aggregate-vs-projected-columns"><i class="icofont-laika">&#xef71;</i></a></h2>
        <p>Spark&#39;s <code>Dataset</code> do not distinguish between columns created from aggregate operations, 
        such as summing or averaging, and simple projections/selections. 
        This is problematic when you start mixing the two.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="keyword">import</span><span> </span><span class="identifier">org</span><span>.</span><span class="identifier">apache</span><span>.</span><span class="identifier">spark</span><span>.</span><span class="identifier">sql</span><span>.</span><span class="identifier">functions</span><span>.</span><span class="identifier">sum</span></code></pre>
        <pre><code class="nohighlight"><span class="identifier">ds</span><span>.</span><span class="identifier">select</span><span>(</span><span class="identifier">sum</span><span>(</span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>), </span><span class="identifier">$</span><span class="string-literal">&quot;i&quot;</span><span>*</span><span class="number-literal">2</span><span>)
</span><span class="comment">// org.apache.spark.sql.AnalysisException: grouping expressions sequence is empty, and &#39;i&#39; is not an aggregate function. Wrap &#39;(sum(i) AS `sum(i)`)&#39; in windowing function(s) or wrap &#39;i&#39; in first() (or first_value) if you don&#39;t care which value you get.;
// Aggregate [sum(i#2063L) AS sum(i)#2204L, (i#2063L * cast(2 as bigint)) AS (i * 2)#2205L]
// +- Relation [i#2063L,j#2064] parquet
// 
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:51)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:50)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:296)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:311)
// 	at scala.collection.Iterator.foreach(Iterator.scala:943)
// 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
// 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
// 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
// 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
// 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$13$adapted(CheckAnalysis.scala:311)
// 	at scala.collection.Iterator.foreach(Iterator.scala:943)
// 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
// 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
// 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
// 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
// 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkValidAggregateExpression$1(CheckAnalysis.scala:311)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$16(CheckAnalysis.scala:338)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$16$adapted(CheckAnalysis.scala:338)
// 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
// 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
// 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:338)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
// 	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
// 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
// 	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
// 	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
// 	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
// 	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
// 	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
// 	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
// 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
// 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
// 	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3734)
// 	at org.apache.spark.sql.Dataset.select(Dataset.scala:1454)
// 	at repl.MdocSession$App0$$anonfun$44.apply(TypedDatasetVsSparkDataset.md:173)
// 	at repl.MdocSession$App0$$anonfun$44.apply(TypedDatasetVsSparkDataset.md:173)</span></code></pre>
        <p>In Frameless, mixing the two results in a compilation error.</p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="comment">// To avoid confusing frameless&#39; sum with the standard Spark&#39;s sum
</span><span class="keyword">import</span><span> </span><span class="identifier">frameless</span><span>.</span><span class="identifier">functions</span><span>.</span><span class="identifier">aggregate</span><span>.{</span><span class="identifier">sum</span><span> =&gt; </span><span class="identifier">fsum</span><span>}</span></code></pre>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">select</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)))
</span><span class="comment">// error: polymorphic expression cannot be instantiated to expected type;
//  found   : [Out]frameless.TypedAggregate[repl.MdocSession.App0.Foo,Out]
//  required: frameless.TypedColumn[repl.MdocSession.App0.Foo,?]
// fds.select(fsum(fds(&#39;i)))
//            ^^^^^^^^^^^^^</span></code></pre>
        <p>As the error suggests, we expected a <code>TypedColumn</code> but we got a <code>TypedAggregate</code> instead. </p>
        <p>Here is how you apply an aggregation method in Frameless: </p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">agg</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>))+</span><span class="number-literal">22</span><span>).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// +-----+
// |value|
// +-----+
// |  133|
// +-----+
//</span></code></pre>
        <p>Similarly, mixing projections while aggregating does not make sense, and in Frameless
        you get a compilation error.<br></p>
        <pre class="keep-together pdf epub"><code class="nohighlight"><span class="identifier">fds</span><span>.</span><span class="identifier">agg</span><span>(</span><span class="identifier">fsum</span><span>(</span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)), </span><span class="identifier">fds</span><span>(</span><span class="symbol-literal">&#39;i</span><span>)).</span><span class="identifier">show</span><span>().</span><span class="identifier">run</span><span>()
</span><span class="comment">// error: polymorphic expression cannot be instantiated to expected type;
//  found   : [A]frameless.TypedColumn[repl.MdocSession.App0.Foo,A]
//  required: frameless.TypedAggregate[repl.MdocSession.App0.Foo,?]
// fds.agg(fsum(fds(&#39;i)), fds(&#39;i)).show().run()
//                        ^^^^^^^</span></code></pre>

      </main>

    </div>

  </body>
</html>